{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pylab as plt\n",
    "import csv\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VSModel():\n",
    "    \n",
    "    def __init__(self, theta_1=0, theta_2=2, alpha=0.8, beta=0.2, phi=0.8, method='exp_mean', use_h = False, lr = 0.001, \n",
    "                 schedule = None, schedule_factor=0.9, verbose=5, is_sgd=False):\n",
    "        '''\n",
    "        theta_1 - theta of the 1st line, usually 0 < theta_1 < 1\n",
    "        theta_2 - theta of the 2nd line, usually > 1\n",
    "        alpha - coefficient for exponential mean\n",
    "        method - method to extrapolate z's values, possible values: exp_mean, exp_trend, holts, damped\n",
    "        h - use h for predicting or not (not for exp_mean)\n",
    "        lr - learning rate\n",
    "        schedule - function for schedule of the probability to add true next value or predicted value\n",
    "        schedule_factor - parameter for the schedule\n",
    "        verbose - frequency for verbose output\n",
    "        sgd - if True - use stochastic gradient descent, else - compute full loss and then make gd step\n",
    "        '''\n",
    "        self.is_sgd = is_sgd\n",
    "        \n",
    "        self.logs = dict()\n",
    "        self.logs['err_log'] = []\n",
    "        self.logs['mean_err_log'] = []\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.test_size = None\n",
    "        \n",
    "        self.theta_1 = Variable(torch.FloatTensor(np.array([theta_1])), requires_grad=True)\n",
    "        self.theta_2 = Variable(torch.FloatTensor(np.array([theta_2])), requires_grad=True)\n",
    "        self.logs['theta_1'] = [theta_1]\n",
    "        self.logs['theta_2'] = [theta_2]\n",
    "        \n",
    "        self.using_method = method\n",
    "        self.use_h = use_h\n",
    "        \n",
    "        self.b_cur = None\n",
    "        self.l_cur = None\n",
    "        \n",
    "        if schedule is None:\n",
    "            self.schedule = self.exp_schedule\n",
    "        else:\n",
    "            self.schedule = schedule\n",
    "        self.schedule_factor = schedule_factor\n",
    "        \n",
    "        if method == 'exp_mean':\n",
    "            self.method = self.exp_mean\n",
    "            self.method_next = self.exp_mean_next\n",
    "            \n",
    "            self.alpha = Variable(torch.FloatTensor(np.array([alpha])), requires_grad=True)\n",
    "            self.logs['alpha'] = [alpha]\n",
    "            \n",
    "        elif method == 'exp_trend':\n",
    "            self.method = self.exp_trend\n",
    "            self.method_next = self.exp_trend_next\n",
    "            \n",
    "            self.alpha = Variable(torch.FloatTensor(np.array([alpha])), requires_grad=True)\n",
    "            self.beta = Variable(torch.FloatTensor(np.array([beta])), requires_grad=True)\n",
    "            self.logs['alpha'] = [alpha]\n",
    "            self.logs['beta'] = [beta]\n",
    "            \n",
    "        elif method == 'holts':\n",
    "            self.method = self.holts\n",
    "            self.method_next = self.holts_next\n",
    "            \n",
    "            self.alpha = Variable(torch.FloatTensor(np.array([alpha])), requires_grad=True)\n",
    "            self.beta = Variable(torch.FloatTensor(np.array([beta])), requires_grad=True)\n",
    "            self.logs['alpha'] = [alpha]\n",
    "            self.logs['beta'] = [beta]\n",
    "            \n",
    "        elif method == 'damped':\n",
    "            self.method = self.damped\n",
    "            self.method_next = self.damped_next\n",
    "            \n",
    "            self.alpha = Variable(torch.FloatTensor(np.array([alpha])), requires_grad=True)\n",
    "            self.beta = Variable(torch.FloatTensor(np.array([beta])), requires_grad=True)\n",
    "            self.phi = Variable(torch.FloatTensor(np.array([phi])), requires_grad=True)\n",
    "            self.logs['alpha'] = [alpha]\n",
    "            self.logs['beta'] = [beta]\n",
    "            self.logs['phi'] = [phi]\n",
    "            \n",
    "        else:\n",
    "            print(':P')\n",
    "            print('Wrong method')\n",
    "        \n",
    "    def exp_schedule(self, cur_prob, iteration):\n",
    "        '''Schedule for the probabilty, exponential decay'''\n",
    "        return cur_prob * self.schedule_factor\n",
    "        \n",
    "    def exp_mean(self, z):\n",
    "        '''Compute exponential mean with coefficient alpha'''\n",
    "        \n",
    "        exp = Variable(torch.arange(z.shape[0] - 1, -1, -1))\n",
    "        power = torch.pow(1 - self.alpha, exp)\n",
    "        \n",
    "        return torch.sum(self.alpha * power * z)\n",
    "    \n",
    "    def exp_mean_next(self, prev_z, next_z):\n",
    "        return self.alpha * next_z + (1 - self.alpha) * prev_z\n",
    "\n",
    "    def exp_trend(self, z):\n",
    "        self.l_cur = z[0]\n",
    "        self.b_cur = z[1] / z[0]\n",
    "        cur_val = None\n",
    "        \n",
    "\n",
    "        for i in range(len(z)):\n",
    "            cur_val = self.exp_trend_next(z[i])\n",
    "        \n",
    "        return cur_val\n",
    "        \n",
    "    def exp_trend_next(self, prev_z, next_z = None, l=None, b=None, h = 1):\n",
    "        if l is None:\n",
    "            l = self.l_cur\n",
    "        if b is None:\n",
    "            b = self.b_cur\n",
    "\n",
    "        self.b_cur = self.beta * (self.alpha * prev_z + (1 - self.alpha) * l * b) / l + (1 - self.beta) * b\n",
    "        self.l_cur = self.alpha * prev_z + (1 - self.alpha) * l * b\n",
    "\n",
    "        return self.l_cur * torch.pow(self.b_cur, h)\n",
    "        \n",
    "    def holts(self, z):\n",
    "        self.l_cur = z[0]\n",
    "        self.b_cur = z[1] - z[0]\n",
    "        cur_val = None\n",
    "        \n",
    "        for i in range(len(z)):\n",
    "            cur_val = self.holts_next(z[i])\n",
    "        \n",
    "        return cur_val\n",
    "        \n",
    "    def holts_next(self, prev_z, next_z = None, l=None, b=None, h = 1):\n",
    "        if l is None:\n",
    "            l = self.l_cur\n",
    "        if b is None:\n",
    "            b = self.b_cur\n",
    "\n",
    "        self.b_cur = self.beta * ((self.alpha * prev_z + (1 - self.alpha) * (l + b)) - l) + (1 - self.beta) * b\n",
    "        self.l_cur = self.alpha * prev_z + (1 - self.alpha) * (l + b)\n",
    "\n",
    "        return self.l_cur + self.b_cur * h\n",
    "\n",
    "    def damped(self, z):\n",
    "        self.l_cur = z[0]\n",
    "        self.b_cur = z[1] - z[0]\n",
    "        cur_val = None\n",
    "        \n",
    "        for i in range(len(z)):\n",
    "            cur_val = self.damped_next(z[i])\n",
    "        \n",
    "        return cur_val\n",
    "        \n",
    "    def damped_next(self, prev_z, next_z = None, l=None, b=None, h = 1):\n",
    "        if l is None:\n",
    "            l = self.l_cur\n",
    "        if b is None:\n",
    "            b = self.b_cur\n",
    "\n",
    "        self.b_cur = (self.beta * ((self.alpha * prev_z + (1 - self.alpha) * (l + self.phi * b)) - l) +\n",
    "                     (1 - self.beta) * self.phi * b)\n",
    "        \n",
    "        self.l_cur = self.alpha * prev_z + (1 - self.alpha) * (l + self.phi * b)\n",
    "\n",
    "        return self.l_cur + torch.cumsum(torch.pow(self.phi, h), -1) * self.b_cur\n",
    "        \n",
    "    def get_predict(self, em1, em2):\n",
    "        '''Compute predicted value, formulas required, that 0 < theta_1 < 1, theta_2 > 1'''\n",
    "        \n",
    "        w1 = (self.theta_2 - 1) / (self.theta_2 - self.theta_1)\n",
    "        w2 = (1 - self.theta_1) / (self.theta_2 - self.theta_1)\n",
    "        \n",
    "        return w1 * em1 + w2 * em2\n",
    "    \n",
    "    def mae(self, target, predict):\n",
    "        '''Compute mean average error'''\n",
    "        \n",
    "\n",
    "        idx = torch.LongTensor(np.array(np.isnan(predict.data.numpy()), dtype=int))\n",
    "        return torch.mean(torch.abs(predict[idx] - target[idx]))\n",
    "    \n",
    "    def logging(self):\n",
    "        self.logs['theta_1'].append(self.theta_1.data.numpy()[0])\n",
    "        self.logs['theta_2'].append(self.theta_2.data.numpy()[0])\n",
    "        \n",
    "        if 'alpha' in self.logs:\n",
    "            self.logs['alpha'].append(self.alpha.data.numpy()[0])\n",
    "        if 'beta' in self.logs:\n",
    "            self.logs['beta'].append(self.beta.data.numpy()[0])\n",
    "        if 'phi' in self.logs:\n",
    "            self.logs['phi'].append(self.phi.data.numpy()[0])\n",
    "            \n",
    "    def print_graph(self):\n",
    "        clear_output()\n",
    "        plt.plot(np.arange(len(self.logs['theta_1'])), self.logs['theta_1'], label='theta_1')\n",
    "        plt.plot(np.arange(len(self.logs['theta_2'])), self.logs['theta_2'], label='theta_2')\n",
    "        plt.title('Current values: theta_1 = {}, theta_2 = {}'.format(self.theta_1.data.numpy()[0], self.theta_2.data.numpy()[0]))\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "        \n",
    "        for name in ['alpha', 'beta', 'phi']:\n",
    "            if name in self.logs:\n",
    "                plt.plot(np.arange(len(self.logs[name])), self.logs[name], label=name)\n",
    "                plt.title('Current value {} = {}'.format(name, self.logs[name][-1]))\n",
    "                plt.legend(loc='best')\n",
    "                plt.show()\n",
    "                \n",
    "        plt.plot(np.arange(len(self.logs['err_log'])), self.logs['err_log'], label='err_log', alpha=0.3)\n",
    "        plt.plot(np.arange(len(self.logs['mean_err_log'])) * self.test_size, self.logs['mean_err_log'], label='mean_err_log', alpha=0.7)\n",
    "        plt.title('Current values: err_log = {}, mean_err_log = {}'.format(self.logs['err_log'][-1], self.logs['mean_err_log'][-1]))\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "        \n",
    "    def fit(self, data_base, data_train, n_iters, optimizer=torch.optim.Adam, loss_function=None, \n",
    "            constraints_1=True, constraints_2=True, lr=None):\n",
    "        '''\n",
    "        Find optimal theta_1, theta_2 and alpha with gradiend descent\n",
    "        \n",
    "        data_base - data to compute first n thetas\n",
    "        data_train - data to optimize parameters with GD\n",
    "        n_iters - number of epochs\n",
    "        optimizer - which type of GD to use\n",
    "        loss_function - function to compute error, should be writtein on pytorch, syntax: loss_function(target, predict),\n",
    "        default value is mae\n",
    "        constraints_1 - flag to constraint values of thetas\n",
    "        constraints_2 - flag to constraint values of alpha, beta, phi\n",
    "        '''\n",
    "        \n",
    "        if loss_function is None:\n",
    "            loss_function = self.mae\n",
    "        \n",
    "        if lr is None:\n",
    "            lr = self.lr\n",
    "        \n",
    "        #Simple preparing\n",
    "        y_np = np.copy(data_base)\n",
    "        n = len(y_np)\n",
    "        \n",
    "        t_np = np.arange(n) + 1\n",
    "        t_var = Variable(torch.FloatTensor(t_np))\n",
    "        y_var = Variable(torch.FloatTensor(y_np))\n",
    "        \n",
    "        #Compute coefficients A and B \n",
    "        B = 6 / (n ** 2 - 1) * (2 / n * np.sum(t_np * y_np) - (1 + n) / n * np.sum(y_np))\n",
    "        A = 1 / n * np.sum(y_np) - (n + 1) / 2 * B\n",
    "        \n",
    "        B = Variable(torch.FloatTensor(np.array([B])))\n",
    "        A = Variable(torch.FloatTensor(np.array([A])))\n",
    "        \n",
    "        #Compute sequence of z_t \n",
    "        z_var_1 = self.theta_1 * y_var + (1 - self.theta_1) * (A + B * t_var)\n",
    "        z_var_2 = self.theta_2 * y_var + (1 - self.theta_2) * (A + B * t_var)\n",
    "\n",
    "        list_of_params = [self.theta_1, self.theta_2, self.alpha]\n",
    "        if self.using_method in ['exp_trend', 'holts', 'damped']:\n",
    "            list_of_params.append(self.beta)\n",
    "        if self.using_method == 'damped':\n",
    "            list_of_params.append(self.phi)\n",
    "            \n",
    "        opt = optimizer(list_of_params, lr=lr)\n",
    "        targets_np = np.copy(data_train)\n",
    "        targets_var = Variable(torch.FloatTensor(targets_np))\n",
    "        \n",
    "        prob = 1\n",
    "        self.test_size = targets_var.shape[0]\n",
    "        \n",
    "\n",
    "        for it in range(n_iters):\n",
    "\n",
    "            if not self.is_sgd:\n",
    "                full_predict = Variable(torch.FloatTensor(np.zeros(self.test_size)))\n",
    "    \n",
    "            if self.using_method == 'exp_mean' or not self.use_h:\n",
    "                # Compute exponential mean to predict\n",
    "                z1_predict = self.method(z_var_1)\n",
    "                z2_predict = self.method(z_var_2)\n",
    "                \n",
    "                for i in range(targets_var.shape[0]):\n",
    "                    #predict\n",
    "                    predict = self.get_predict(z1_predict, z2_predict)\n",
    "\n",
    "                    #Gradient step\n",
    "                    if self.is_sgd:\n",
    "                        opt.zero_grad()\n",
    "                        loss = loss_function(predict, targets_var[i])\n",
    "                        self.logs['err_log'].append(loss.data.numpy()[0])\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        opt.step()\n",
    "                    else:\n",
    "                        full_predict[i] = predict\n",
    "\n",
    "                    #Limiting values\n",
    "                    if constraints_1:\n",
    "                        if (self.theta_1 < 0).data.numpy():\n",
    "                            self.theta_1 = Variable(torch.FloatTensor(np.array([0.0])), requires_grad=True)\n",
    "                        if (self.theta_1 > 1).data.numpy():\n",
    "                            self.theta_1 = Variable(torch.FloatTensor(np.array([1.0])), requires_grad=True)\n",
    "                        if (self.theta_2 < 1).data.numpy():\n",
    "                            self.theta_2 = Variable(torch.FloatTensor(np.array([2.0])), requires_grad=True)\n",
    "                    \n",
    "                    if constraints_2:    \n",
    "                        if (self.alpha > 1).data.numpy():\n",
    "                            self.alpha = Variable(torch.FloatTensor(np.array([1.0])), requires_grad=True)\n",
    "                        if (self.alpha < 0).data.numpy():\n",
    "                            self.alpha = Variable(torch.FloatTensor(np.array([0.0])), requires_grad=True)\n",
    "\n",
    "                        if 'beta' in self.logs and (self.beta > 1).data.numpy():\n",
    "                            self.beta = Variable(torch.FloatTensor(np.array([1.0])), requires_grad=True)\n",
    "                        if 'beta' in self.logs and (self.beta < 0).data.numpy():\n",
    "                            self.beta = Variable(torch.FloatTensor(np.array([0.0])), requires_grad=True)\n",
    "\n",
    "                        if 'phi' in self.logs and (self.phi > 1).data.numpy():\n",
    "                            self.phi = Variable(torch.FloatTensor(np.array([0.98])), requires_grad=True)\n",
    "                        if 'phi' in self.logs and (self.phi < 0).data.numpy():\n",
    "                            self.phi = Variable(torch.FloatTensor(np.array([0.0])), requires_grad=True)\n",
    "                            \n",
    "                    self.logging()\n",
    "                    \n",
    "                    # Remember last values\n",
    "                    prev_z1 = z1_predict \n",
    "                    prev_z2 = z2_predict\n",
    "\n",
    "                    # Compute next value in z\n",
    "                    if np.random.binomial(n=1, p=prob, size=1) == 0:\n",
    "                        next_z1 = self.theta_1 * predict + (1 - self.theta_1) * (A + B * (n + i + 1))\n",
    "                        next_z2 = self.theta_2 * predict + (1 - self.theta_2) * (A + B * (n + i + 1))\n",
    "                    else:\n",
    "                        next_z1 = self.theta_1 * targets_var[i] + (1 - self.theta_1) * (A + B * (n + i + 1))\n",
    "                        next_z2 = self.theta_2 * targets_var[i] + (1 - self.theta_2) * (A + B * (n + i + 1))\n",
    "\n",
    "                    #Compute next exp mean\n",
    "                    z1_predict = self.method_next(prev_z1, next_z1)\n",
    "                    z2_predict = self.method_next(prev_z2, next_z2)\n",
    "                \n",
    "                \n",
    "                if not self.is_sgd:\n",
    "                    opt.zero_grad()\n",
    "                    loss = loss_function(full_predict, targets_var)\n",
    "                    self.logs['err_log'].append(loss.data.numpy()[0])\n",
    "                    self.logs['mean_err_log'].append(loss.data.numpy()[0])\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    opt.step()\n",
    "                \n",
    "                #decrease prob\n",
    "                prob = self.schedule(prob, i)\n",
    "                if self.is_sgd:\n",
    "                    self.logs['mean_err_log'].append(np.mean(self.logs['err_log'][-self.test_size : ]))\n",
    "\n",
    "            else:\n",
    "                predicts = Variable(torch.FloatTensor(np.zeros(len(targets_var))))            \n",
    "                z1_predict = self.method(z_var_1)\n",
    "                z2_predict = self.method(z_var_2)\n",
    "\n",
    "                predicts[0] = self.get_predict(z1_predict, z2_predict)\n",
    "                \n",
    "                h = Variable(torch.FloatTensor(np.arange(1, len(targets_var))))\n",
    "                z1_predict = self.method_next(z1_predict, h = h)\n",
    "                z2_predict = self.method_next(z2_predict, h = h)\n",
    "                predicts[1:] = self.get_predict(z1_predict, z2_predict)\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss = loss_function(targets_var, predicts)\n",
    "                self.logs['err_log'].append(loss.data.numpy()[0])\n",
    "                self.logs['mean_err_log'].append(loss.data.numpy()[0])\n",
    "                loss.backward(retain_graph=True)\n",
    "                opt.step()\n",
    "                \n",
    "                if constraints_1:\n",
    "                    if (self.theta_1 < 0).data.numpy():\n",
    "                        self.theta_1 = Variable(torch.FloatTensor(np.array([0.0])), requires_grad=True)\n",
    "                    if (self.theta_1 > 1).data.numpy():\n",
    "                        self.theta_1 = Variable(torch.FloatTensor(np.array([1.0])), requires_grad=True)\n",
    "                    if (self.theta_2 < 1).data.numpy():\n",
    "                        self.theta_2 = Variable(torch.FloatTensor(np.array([2.0])), requires_grad=True)\n",
    "                \n",
    "                if constraints_2:\n",
    "                    if (self.alpha > 1).data.numpy():\n",
    "                        self.alpha = Variable(torch.FloatTensor(np.array([1.0])), requires_grad=True)\n",
    "                    if (self.alpha < 0).data.numpy():\n",
    "                        self.alpha = Variable(torch.FloatTensor(np.array([0.0])), requires_grad=True)\n",
    "                        \n",
    "                    if 'beta' in self.logs and (self.beta > 1).data.numpy():\n",
    "                        self.beta = Variable(torch.FloatTensor(np.array([1.0])), requires_grad=True)\n",
    "                    if 'beta' in self.logs and (self.beta < 0).data.numpy():\n",
    "                        self.beta = Variable(torch.FloatTensor(np.array([0.0])), requires_grad=True)\n",
    "                        \n",
    "                    if 'phi' in self.logs and (self.phi > 1).data.numpy():\n",
    "                        self.phi = Variable(torch.FloatTensor(np.array([0.98])), requires_grad=True)\n",
    "                    if 'phi' in self.logs and (self.phi < 0).data.numpy():\n",
    "                        self.phi = Variable(torch.FloatTensor(np.array([0.0])), requires_grad=True)\n",
    "            \n",
    "                self.logging()\n",
    "              ### for printing progress of optimization  \n",
    "#             if it % self.verbose == 0 or it == n_iters - 1:\n",
    "#                 self.print_graph()\n",
    "                    \n",
    "        return self.logs['err_log']\n",
    "                \n",
    "    def predict(self, data_base, n_iters, use_h = None):\n",
    "        '''\n",
    "        Predict n_iters values\n",
    "        n_iters - number of values to predict\n",
    "        use_h - use h for prediction or predict iteratively\n",
    "        '''\n",
    "        \n",
    "        if use_h is None:\n",
    "            use_h = self.use_h\n",
    "        \n",
    "        y_np = np.copy(data_base)\n",
    "        n = len(y_np)\n",
    "\n",
    "        t_np = np.arange(n) + 1\n",
    "        t_var = Variable(torch.FloatTensor(t_np))\n",
    "        y_var = Variable(torch.FloatTensor(y_np))\n",
    "\n",
    "        B = 6 / (n ** 2 - 1) * (2 / n * np.sum(t_np * y_np) - (1 + n) / n * np.sum(y_np))\n",
    "        A = 1 / n * np.sum(y_np) - (n + 1) / 2 * B\n",
    "\n",
    "        B = Variable(torch.FloatTensor(np.array([B])))\n",
    "        A = Variable(torch.FloatTensor(np.array([A])))\n",
    "\n",
    "        z_var_1 = self.theta_1 * y_var + (1 - self.theta_1) * (A + B * t_var)\n",
    "        z_var_2 = self.theta_2 * y_var + (1 - self.theta_2) * (A + B * t_var)\n",
    "\n",
    "        z1_predict = self.method(z_var_1)\n",
    "        z2_predict = self.method(z_var_2)\n",
    "\n",
    "        predicted_values = []\n",
    "\n",
    "        \n",
    "        if self.using_method == 'exp_mean' or not use_h:\n",
    "            for i in range(n_iters):\n",
    "                predict = self.get_predict(z1_predict, z2_predict)\n",
    "            \n",
    "                predicted_values.append(predict.data.numpy()[0])\n",
    "                # Remember last values\n",
    "                prev_z1 = z1_predict \n",
    "                prev_z2 = z2_predict\n",
    "\n",
    "                # Compute next value in z\n",
    "\n",
    "                next_z1 = self.theta_1 * predict + (1 - self.theta_1) * (A + B * (n + i + 1))\n",
    "                next_z2 = self.theta_2 * predict + (1 - self.theta_2) * (A + B * (n + i + 1))\n",
    "\n",
    "                #Compute next exp mean\n",
    "                z1_predict = self.method_next(prev_z1, next_z1)\n",
    "                z2_predict = self.method_next(prev_z2, next_z2)\n",
    "\n",
    "            return np.array(predicted_values)\n",
    "        \n",
    "        else:\n",
    "            predicts = np.zeros(n_iters)\n",
    "            z1_predict = self.method(z_var_1)\n",
    "            z2_predict = self.method(z_var_2)\n",
    "            predicts[0] = self.get_predict(z1_predict, z2_predict).data.numpy()\n",
    "            \n",
    "            h = Variable(torch.FloatTensor(np.arange(1, n_iters)))\n",
    "            z1_predict = self.method_next(z1_predict, h = h)\n",
    "            z2_predict = self.method_next(z2_predict, h = h)\n",
    "            predicts[1:] = self.get_predict(z1_predict, z2_predict).data.numpy()\n",
    "            \n",
    "            return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example\n",
    "hor = 3\n",
    "series = np.arange(1,40,2)\n",
    "test = np.split(series, [-hor])[1].copy()\n",
    "train = np.split(series, [-hor])[0].copy() \n",
    "try:\n",
    "    train1,train2 = np.split(train, 2)\n",
    "except ValueError:\n",
    "    train1 = np.split(train, [len(train)//2+1])[0].copy()\n",
    "    train2 = np.split(train, [len(train)//2+1])[1].copy()  \n",
    "# use Holt's trend method with non-iterative forecasts\n",
    "model = VSModel(verbose=1000,method='holts',lr=0.0015,use_h=True)\n",
    "logs = model.fit(train1,train2,n_iters = 1000)\n",
    "predict = model.predict(train,hor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values: [ 35.          36.78452301  38.39121246]\n",
      "true values: [35 37 39]\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted values:\",predict)\n",
    "print(\"true values:\",test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
